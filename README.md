# Enterprise Document Intelligence Assistant

This project is a Retrieval Augmented Generation (RAG) based AI document Q&A system. It allows users to upload PDF documents, ask questions about their content, and receive answers generated by an AI model (Google Gemini), with documents and embeddings managed by Pinecone. The system includes an API backend built with FastAPI, an optional interactive web UI built with Streamlit, and monitoring capabilities via Weights & Biases. The application is designed to be containerized using Docker.

## Features

- **Document Q&A**: Ask natural language questions about your PDF documents
- **RAG Pipeline**: Utilizes a Retrieval Augmented Generation pipeline for contextually relevant answers
- **PDF Processing**: Ingests, chunks, and embeds PDF documents
- **Scalable Vector Storage**: Uses Pinecone for efficient storage and retrieval of document embeddings
- **AI Powered**: Leverages Google Gemini series models for embeddings and text generation
- **FastAPI Backend**: Provides a robust API for Q&A functionality
- **Interactive Streamlit UI**: User-friendly web interface with chat-based Q&A and file upload functionality
- **Cloud Hosted**: Live demo available at [enterprise-doc-assistant.onrender.com](https://enterprise-doc-assistant.onrender.com)
- **Dockerized**: Fully containerized for consistent deployment and scalability
- **Monitoring**: Integrated with Weights & Biases for tracking API calls and performance

## Technologies Used

- **Programming Language**: Python 3.8+
- **Core AI/ML Framework**: LangChain
- **LLM & Embeddings**: Google Gemini (via langchain-google-genai)
- **Vector Database**: Pinecone
- **API Framework**: FastAPI
- **Web UI Framework**: Streamlit
- **Containerization**: Docker
- **Monitoring**: Weights & Biases (wandb)
- **Environment Management**: python-dotenv
- **PDF Processing**: pypdf

## Project Structure (Key Files)

```
.
â”œâ”€â”€ docs/                     # Directory for sample/uploaded PDF documents
â”œâ”€â”€ .env.example              # Example environment file (DO NOT COMMIT ACTUAL .env FILE)
â”œâ”€â”€ .gitignore                # Specifies intentionally untracked files that Git should ignore
â”œâ”€â”€ Dockerfile                # Docker configuration for the application
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ main_api.py               # FastAPI backend application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ ui_app.py                 # Streamlit UI application with chat interface
â””â”€â”€ ...                       # Other configuration or script files
```

## Prerequisites

Before you begin, ensure you have the following installed/configured:

- **Python**: Version 3.8 or higher
- **Git**: For cloning the repository
- **Docker Desktop**: For building and running Docker containers

### API Keys & Accounts Required

- **Pinecone Account**:
  - API Key
  - Environment/Region (e.g., us-east-1)
  - An existing Pinecone Index name or a name for a new one (the script can create it)

- **Google AI API Key**:
  - An API key with access to Gemini models (e.g., from Google AI Studio or a Google Cloud Project with Vertex AI enabled)

- **Weights & Biases Account** (Optional, if using monitoring):
  - API Key
  - Project Name

- **AWS Account & AWS CLI** (Optional, for future cloud deployment):
  - Configured with necessary permissions if you plan to deploy to AWS later

## Live Demo

ðŸš€ **Try the application now**: [https://enterprise-doc-assistant.onrender.com](https://enterprise-doc-assistant.onrender.com)

The live demo includes:
- Interactive chat-based interface
- PDF document upload functionality
- Real-time document processing and embedding
- Contextual Q&A with source attribution

*Note: The hosted version uses a shared knowledge base. For production use with sensitive documents, consider deploying your own instance.*

## Setup and Installation (Local)

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPOSITORY_NAME.git
cd YOUR_REPOSITORY_NAME
```

### 2. Create and Activate a Python Virtual Environment

**On macOS/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**On Windows:**
```bash
python -m venv .venv
.\.venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Set Up Environment Variables

Create a file named `.env` in the root of the project directory. You can copy `.env.example` (if provided in the repository) and rename it to `.env`.

**IMPORTANT**: The `.env` file should NEVER be committed to Git. Ensure it's listed in your `.gitignore` file.

Add your API keys and configuration to the `.env` file. See `.env.example` for required variables:

```env
# Example content for .env (replace with your actual values)
GOOGLE_API_KEY="YOUR_GOOGLE_AI_API_KEY_HERE"
PINECONE_API_KEY="YOUR_PINECONE_API_KEY_HERE"
PINECONE_ENVIRONMENT="your-pinecone-region-e.g.-us-east-1"
PINECONE_INDEX_NAME="your-chosen-pinecone-index-name"
WANDB_API_KEY="YOUR_WANDB_API_KEY_HERE" # Optional
WANDB_PROJECT_NAME="your-wandb-project-name" # Optional, e.g., gemini-doc-assistant
```

### 5. Initial Document Setup (for Pinecone Population)

The application is designed to populate the Pinecone index if it's new/empty and PDF documents are found in the `docs/` directory during startup.

- Place a few sample PDF files in the `docs/` directory for initial testing and index population
- **Note**: For a large number of documents, a separate, dedicated ingestion script is recommended

## Running the Application Locally

### 1. API Backend (main_api.py)

This provides the core Q&A functionality via an API.

1. Ensure your `.env` file is configured
2. Run the FastAPI application using Uvicorn:

```bash
uvicorn main_api:app --reload --port 8000
```

- The API will be accessible at `http://127.0.0.1:8000`
- Interactive API documentation (Swagger UI) will be available at `http://127.0.0.1:8000/docs`

### 2. Streamlit UI (ui_app.py)

This provides an interactive web interface with chat-based Q&A and document upload functionality.

**Features:**
- Chat-based interface similar to ChatGPT
- Drag-and-drop PDF upload in sidebar
- Real-time document processing
- Source attribution for answers
- Session-based chat history
- Responsive design

**To run locally:**
1. Ensure your `.env` file is configured
2. Run the Streamlit application:

```bash
streamlit run ui_app.py
```

- The UI will typically be accessible at `http://localhost:8501` (Streamlit will show the exact URL)

**Usage:**
1. Upload PDF documents using the sidebar
2. Click "Process Uploaded Files" to add them to the knowledge base
3. Start asking questions in the chat interface
4. View source documents for each answer

## Docker Usage

### 1. Build the Docker Image

The Dockerfile should be configured to run either `main_api.py` or `ui_app.py`. Adjust the CMD instruction in the Dockerfile as needed.

```bash
docker build -t gemini-doc-assistant-app .
```

### 2. Run the Docker Container (Local)

Pass environment variables to the container at runtime.

**Example for main_api.py (API on port 8000):**

```bash
docker run -p 8000:8000 \
  -e GOOGLE_API_KEY="YOUR_GOOGLE_AI_API_KEY_HERE" \
  -e PINECONE_API_KEY="YOUR_PINECONE_API_KEY_HERE" \
  -e PINECONE_ENVIRONMENT="your-pinecone-region-e.g.-us-east-1" \
  -e PINECONE_INDEX_NAME="your-chosen-pinecone-index-name" \
  -e WANDB_API_KEY="YOUR_WANDB_API_KEY_HERE" \
  -e WANDB_PROJECT_NAME="your-wandb-project-name" \
  gemini-doc-assistant-app
```

Access the API at `http://localhost:8000/docs`.

**Example for ui_app.py (UI on port 8501):**
(Ensure Dockerfile CMD is `streamlit run ui_app.py --server.port 8501 --server.address 0.0.0.0`)

```bash
docker run -p 8501:8501 \
  -e GOOGLE_API_KEY="YOUR_GOOGLE_AI_API_KEY_HERE" \
  -e PINECONE_API_KEY="YOUR_PINECONE_API_KEY_HERE" \
  -e PINECONE_ENVIRONMENT="your-pinecone-region-e.g.-us-east-1" \
  -e PINECONE_INDEX_NAME="your-chosen-pinecone-index-name" \
  -e WANDB_API_KEY="YOUR_WANDB_API_KEY_HERE" \
  -e WANDB_PROJECT_NAME="your-wandb-project-name" \
  gemini-doc-assistant-app
```

Access the UI at `http://localhost:8501`.

## Cloud Deployment

### Render Deployment

The application is currently deployed on Render at [https://enterprise-doc-assistant.onrender.com](https://enterprise-doc-assistant.onrender.com).

**To deploy your own instance on Render:**

1. Fork this repository
2. Connect your GitHub repository to Render
3. Create a new Web Service
4. Configure environment variables in Render dashboard:
   - `GOOGLE_API_KEY`
   - `PINECONE_API_KEY`
   - `PINECONE_ENVIRONMENT`
   - `PINECONE_INDEX_NAME`
   - `WANDB_API_KEY` (optional)
   - `WANDB_PROJECT_NAME` (optional)
5. Set build command: `pip install -r requirements.txt`
6. Set start command: `streamlit run ui_app.py --server.port $PORT --server.address 0.0.0.0`

## Weights & Biases Monitoring

If configured, the application logs interactions (queries, answers, processing times, errors) to your specified Weights & Biases project.

Monitor application performance and usage on your W&B dashboard.

## Streamlit UI Features

The Streamlit UI (`ui_app.py`) provides a comprehensive document Q&A experience:

### Chat Interface
- **Interactive Chat**: ChatGPT-style conversation interface
- **Session Memory**: Maintains chat history during your session
- **Real-time Responses**: Streaming-style response display with loading indicators

### Document Management
- **Drag & Drop Upload**: Easy PDF file upload via sidebar
- **Batch Processing**: Upload and process multiple PDF files simultaneously
- **Smart Deduplication**: Prevents reprocessing of already uploaded files in the same session
- **Progress Tracking**: Visual feedback during document processing

### Q&A Capabilities
- **Contextual Answers**: Responses based on uploaded document content
- **Source Attribution**: Shows which documents were used to generate each answer
- **Retrieval Transparency**: Displays source documents when available
- **Error Handling**: Graceful error messages for failed queries

### Technical Features
- **Component Caching**: Optimized performance with Streamlit's caching system
- **Resource Management**: Automatic cleanup of temporary uploaded files
- **Environment Validation**: Checks for required API keys and configurations

**Note for Multi-User Environments**: The current implementation uses a shared Pinecone index, meaning all users contribute to and query from the same knowledge base. For production deployments requiring data isolation, consider implementing user namespaces or separate indexes.

## Troubleshooting

- **Environment Variables**: Ensure `.env` is correctly formatted and all required variables are present. Values loaded by the script can be verified with print statements if issues persist.
- **API Key Issues**: Double-check that all API keys are correct and have the necessary permissions.
- **Docker Issues**: Ensure Docker Desktop is running. Check `docker build` and `docker run` command outputs for errors. Use `docker logs <container_id>` for runtime errors.

## Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## License

This project is licensed under the MIT License. (Consider adding a LICENSE file with the MIT License text)
